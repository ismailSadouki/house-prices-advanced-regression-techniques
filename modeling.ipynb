{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe4152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1022, 253)\n",
      "Test shape: (438, 253)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.769036</td>\n",
       "      <td>0.524097</td>\n",
       "      <td>0.192386</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.038646</td>\n",
       "      <td>-0.586579</td>\n",
       "      <td>1.016985</td>\n",
       "      <td>-0.870390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764812</td>\n",
       "      <td>-1.871658</td>\n",
       "      <td>-1.528533</td>\n",
       "      <td>-0.512048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.773486</td>\n",
       "      <td>0.378167</td>\n",
       "      <td>0.969814</td>\n",
       "      <td>0.069493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.128164</td>\n",
       "      <td>0.105124</td>\n",
       "      <td>-0.234210</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.887117</td>\n",
       "      <td>0.537425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.465322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.769036</td>\n",
       "      <td>0.732724</td>\n",
       "      <td>0.493162</td>\n",
       "      <td>1.406400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.801443</td>\n",
       "      <td>0.416810</td>\n",
       "      <td>1.018377</td>\n",
       "      <td>-0.870390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.128164</td>\n",
       "      <td>-1.437438</td>\n",
       "      <td>0.256074</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.534806</td>\n",
       "      <td>0.062763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   -0.769036     0.524097  0.192386     0.487952          1.0  -0.038646   \n",
       "1    0.764812    -1.871658 -1.528533    -0.512048          0.0   0.773486   \n",
       "2    0.128164     0.105124 -0.234210     0.487952          0.0   0.887117   \n",
       "3   -0.769036     0.732724  0.493162     1.406400          0.0   0.801443   \n",
       "4    0.128164    -1.437438  0.256074     0.487952          1.0   0.534806   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLw  \\\n",
       "0     -0.586579    1.016985   -0.870390         0.0  ...             0.0   \n",
       "1      0.378167    0.969814    0.069493         0.0  ...             0.0   \n",
       "2      0.537425    0.000000   -0.465322         0.0  ...             0.0   \n",
       "3      0.416810    1.018377   -0.870390         0.0  ...             0.0   \n",
       "4      0.062763    0.000000   -0.002587         0.0  ...             1.0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0           0.0           0.0          1.0                    0.0   \n",
       "1           0.0           0.0          1.0                    0.0   \n",
       "2           0.0           0.0          0.0                    0.0   \n",
       "3           0.0           0.0          1.0                    0.0   \n",
       "4           0.0           0.0          0.0                    0.0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                    0.0                   0.0                   0.0   \n",
       "1                    0.0                   0.0                   0.0   \n",
       "2                    0.0                   0.0                   0.0   \n",
       "3                    0.0                   0.0                   0.0   \n",
       "4                    0.0                   0.0                   0.0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                   1.0                    0.0  \n",
       "1                   1.0                    0.0  \n",
       "2                   1.0                    0.0  \n",
       "3                   1.0                    0.0  \n",
       "4                   1.0                    0.0  \n",
       "\n",
       "[5 rows x 253 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load train and test sets\n",
    "X_train_final = pd.read_csv(\"X_train_final.csv\")\n",
    "X_test_final = pd.read_csv(\"X_test_final.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n",
    "\n",
    "# Optional: check shapes and first few rows\n",
    "print(\"Train shape:\", X_train_final.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)\n",
    "X_train_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56e313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================ULTIMATE BENCHMARK WITH TRAIN SCORE & BEST TRACKING=========================\n",
      "Previous Best Test RMSE: 0.11673 (or never set)\n",
      "\n",
      "\n",
      "LinearRegression   → No tuning → TRAIN: 0.09256315651264356 | TEST: 0.12821133826217523\n",
      "\n",
      "SGDRegressor       → CV: nan → TRAIN: 0.14195047063879782 | TEST: 0.15784602539395737\n",
      "\n",
      "Ridge              → CV: nan → TRAIN: 0.09256317798727669 | TEST: 0.1281924859324048\n",
      "\n",
      "LassoCV            → No tuning → TRAIN: 0.10797901533833022 | TEST: 0.11757852714449415\n",
      "\n",
      "ElasticNet         → CV: nan → TRAIN: 0.09819074490177672 | TEST: 0.11943526828379644\n",
      "\n",
      "TweedieRegressor   → CV: nan → TRAIN: 0.09672155719486764 | TEST: 0.12946181020651357\n",
      "\n",
      "BayesianRidge      → No tuning → TRAIN: 0.10426242973015354 | TEST: 0.12160578500410768\n",
      "\n",
      "══════════════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "                                 FINAL LEADERBOARD — TRAIN + TEST + TRACKING                                  \n",
      "══════════════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "                 Model  Train RMSE  Test RMSE  Prev Test RMSE   CV RMSE\n",
      "Rank                                                                   \n",
      "1              LassoCV     0.10798    0.11758         0.11758  built-in\n",
      "2           ElasticNet     0.09819    0.11944         0.11944       NaN\n",
      "3        BayesianRidge     0.10426    0.12161         0.12161       N/A\n",
      "4                Ridge     0.09256    0.12819         0.12819       NaN\n",
      "5     LinearRegression     0.09256    0.12821         0.12821       N/A\n",
      "6     TweedieRegressor     0.09672    0.12946         0.12946       NaN\n",
      "7         SGDRegressor     0.14195    0.15785         0.15785       NaN\n",
      "\n",
      "██████████████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      "WINNER → LassoCV\n",
      "Test RMSE = 0.11758 | Train RMSE = 0.10798\n",
      "No improvement (previous best: 0.11673)\n",
      "██████████████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Full results saved: benchmark_results_20251210_190949.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, ElasticNet, HuberRegressor,\n",
    "    RANSACRegressor, TheilSenRegressor, LassoCV, QuantileRegressor,\n",
    "    TweedieRegressor, OrthogonalMatchingPursuit, SGDRegressor\n",
    ")\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# ULTIMATE BENCHMARK — WITH TRAIN SCORE + TRACKS BEST TEST SCORE EVER\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False, greater_is_better=False)\n",
    "\n",
    "# 2. File to save best score\n",
    "BEST_SCORE_FILE = \"best_test_rmse.txt\"\n",
    "\n",
    "def load_best_score():\n",
    "    if os.path.exists(BEST_SCORE_FILE):\n",
    "        with open(BEST_SCORE_FILE, 'r') as f:\n",
    "            return float(f.read().strip())\n",
    "    return float('inf')\n",
    "\n",
    "def save_best_score(score):\n",
    "    with open(BEST_SCORE_FILE, 'w') as f:\n",
    "        f.write(str(score))\n",
    "\n",
    "previous_best = load_best_score()\n",
    "\n",
    "# History\n",
    "HIST_FILE = \"model_history.csv\"\n",
    "\n",
    "# Load previous history if exists\n",
    "if os.path.exists(HIST_FILE):\n",
    "    history_df = pd.read_csv(HIST_FILE, index_col=\"Model\")\n",
    "else:\n",
    "    history_df = pd.DataFrame(columns=[\"Model\", \"Prev_Test_RMSE\"]).set_index(\"Model\")\n",
    "\n",
    "\n",
    "# 3. ALL YOUR MODELS — one line each\n",
    "'''\n",
    "Model Name : (estimator_object, hyperparameter_grid, scaled_flag),\n",
    "scaled_flag = True → model expects scaled input features (e.g., Ridge, Lasso, ElasticNet).\n",
    "None or empty {} → no GridSearch; fit directly.\n",
    "\n",
    "'''\n",
    "MODELS = {\n",
    "    \"LinearRegression\":   (LinearRegression(),                                      {},                                                False),\n",
    "    \"SGDRegressor\": (SGDRegressor(max_iter=5000, tol=1e-3, random_state=42), {\n",
    "          \"alpha\": [1e-4, 1e-3, 1e-2],        # Regularization strength\n",
    "        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "        \"eta0\": [0.01, 0.1, 0.5],           # Initial learning rate\n",
    "        \"loss\": [\"squared_error\", \"huber\"]  # Loss function\n",
    "    }, True),\n",
    "    \"Ridge\":              (Ridge(random_state=42),                                  {\"alpha\": np.logspace(-3, 3, 10)},                 True),\n",
    "    \"LassoCV\":            (LassoCV(alphas=np.logspace(-4, 1, 20), cv=5, max_iter=10000, random_state=42, n_jobs=-1), None, True),\n",
    "    \"ElasticNet\":         (ElasticNet(max_iter=5000, random_state=42),               {\"alpha\": np.logspace(-3, 2, 10),\n",
    "                                                                                      \"l1_ratio\": np.linspace(0.1, 1.0, 6)},            True),\n",
    "    #\"HuberRegressor\":     (HuberRegressor(max_iter=2000),                            {\"epsilon\": [1.2, 1.35, 1.5, 1.8],\"alpha\": [1e-4, 1e-3, 1e-2, 0.1]},True),\n",
    "    #\"QuantileRegressor\": (QuantileRegressor(quantile=0.5, solver='highs'), {\"alpha\": [0.0, 0.01, 0.1, 1.0],\n",
    "     #                                                                   \"quantile\": [0.25, 0.5, 0.75]}, True),\n",
    "    \"TweedieRegressor\": (TweedieRegressor(), {\"power\": [0, 1, 1.5, 2],\"alpha\": [0.0, 0.01, 0.1, 1.0]}, True),\n",
    "    #\"OrthogonalMatchingPursuit\": (OrthogonalMatchingPursuit(), {\"n_nonzero_coefs\": [5, 10, 15, 20, None]}, True),\n",
    "    #\"RANSAC_Huber\":       (RANSACRegressor(estimator=HuberRegressor(max_iter=2000),min_samples=0.7, residual_threshold=1.5, random_state=42),{\"min_samples\": [0.5, 0.7, 0.9],\"residual_threshold\": [0.5, 1.0, 1.5, 2.0]},       True),\n",
    "    #\"TheilSen\":           (TheilSenRegressor(random_state=42, n_jobs=-1),           {\"max_subpopulation\": [1000, 3000, 5000]},         True),\n",
    "    #\"PLSRegression\":      (PLSRegression(),                                         {\"n_components\": [2, 5, 10, 20, min(40, X_train.shape[1])]}, True),\n",
    "    #\"KNeighborsRegressor\":(KNeighborsRegressor(),                                    {\"n_neighbors\": [3, 5, 7, 10, 15],\n",
    "    #                                                                                  \"weights\": [\"uniform\", \"distance\"],\n",
    "    #                                                                                  \"p\": [1, 2]},                                      True),\n",
    "    #\"RandomForest\": (RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1),\n",
    "    #                 {\"max_depth\": [None, 5, 10, 20]}, False),\n",
    "    #\"GradientBoosting\": (GradientBoostingRegressor(random_state=42),\n",
    "    #                     {\"n_estimators\": [100, 200], \"learning_rate\": [0.05, 0.1], \"max_depth\": [3,5]}, False),\n",
    "    #\"XGBoost\": (XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "    #            {\"n_estimators\": [100, 200], \"learning_rate\": [0.05, 0.1], \"max_depth\": [3,5]}, False),\n",
    "    \"BayesianRidge\": (BayesianRidge(), {}, True),\n",
    "    #\"GPR\": (GaussianProcessRegressor(), {}, True) # not Completed\n",
    "}\n",
    "\n",
    "print(\"ULTIMATE BENCHMARK WITH TRAIN SCORE & BEST TRACKING\".center(100, \"=\"))\n",
    "print(f\"Previous Best Test RMSE: {previous_best:.5f} (or never set)\\n\")\n",
    "\n",
    "results = []\n",
    "current_best = float('inf')\n",
    "\n",
    "\"\"\"\n",
    "For each model:\n",
    "\n",
    "1. Uses scaled or unscaled features based on the scaled flag.\n",
    "\n",
    "2. If hyperparameters exist, runs GridSearchCV with 5-fold CV to find the best.\n",
    "\n",
    "3. Otherwise, fits the model directly.\n",
    "\"\"\"\n",
    "\n",
    "for name, (estimator, params, scaled) in MODELS.items():\n",
    "    # Needs special Attention Latter\n",
    "    X_tr = X_train_final if scaled else X_train_final\n",
    "    X_te = X_test_final  if scaled else X_test_final\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{name:18} → \", end=\"\")\n",
    "    \n",
    "    if params is None or len(params) == 0:\n",
    "        model = estimator.fit(X_tr, y_train_log)\n",
    "        cv_rmse = \"N/A\" if name != \"LassoCV\" else \"built-in\"\n",
    "        best_params = \"default\" if name != \"LassoCV\" else f\"α={model.alpha_:.2e}\"\n",
    "        print(\"No tuning\", end=\" → \")\n",
    "    else:\n",
    "        grid = GridSearchCV(estimator, params, scoring=rmse_scorer, cv=5, n_jobs=-1, refit=True)\n",
    "        grid.fit(X_tr, y_train_log)\n",
    "        model = grid.best_estimator_\n",
    "        cv_rmse = -grid.best_score_\n",
    "        best_params = grid.best_params_\n",
    "        print(f\"CV: {cv_rmse:.5f}\", end=\" → \")\n",
    "    \n",
    "    # Train RMSE\n",
    "    train_pred = model.predict(X_tr)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_log, train_pred))\n",
    "    \n",
    "    # Test RMSE\n",
    "    test_pred = model.predict(X_te)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_log, test_pred))\n",
    "    \n",
    "    print(f\"TRAIN: {train_rmse} | TEST: {test_rmse}\")\n",
    "    \n",
    "    if test_rmse < current_best:\n",
    "        current_best = test_rmse\n",
    "    \n",
    "    # Previous RMSE\n",
    "    prev_rmse = history_df.loc[name, \"Prev_Test_RMSE\"] if name in history_df.index else np.nan\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Train RMSE\": train_rmse,\n",
    "        \"Test RMSE\": test_rmse,\n",
    "        \"CV RMSE\": cv_rmse,\n",
    "        \"Prev Test RMSE\": prev_rmse,\n",
    "        \"Scaled\": \"Yes\" if scaled else \"No\",\n",
    "        \"Best Params\": best_params\n",
    "    })\n",
    "    # Update history\n",
    "    history_df.loc[name, \"Prev_Test_RMSE\"] = test_rmse\n",
    "\n",
    "# FINAL LEADERBOARD\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(\"Test RMSE\").reset_index(drop=True)\n",
    "df.index = df.index + 1\n",
    "df.index.name = \"Rank\"\n",
    "\n",
    "print(\"\\n\" + \"═\" * 110)\n",
    "print(\"FINAL LEADERBOARD — TRAIN + TEST + TRACKING\".center(110))\n",
    "print(\"═\" * 110)\n",
    "print(df[[\"Model\", \"Train RMSE\", \"Test RMSE\", \"Prev Test RMSE\", \"CV RMSE\"]].round(5).to_string())\n",
    "\n",
    "# Check if we beat previous best\n",
    "winner = df.iloc[0]\n",
    "improved = winner[\"Test RMSE\"] < previous_best\n",
    "\n",
    "print(\"\\n\" + \"█\" * 110)\n",
    "print(f\"WINNER → {winner['Model']}\")\n",
    "print(f\"Test RMSE = {winner['Test RMSE']:.5f} | Train RMSE = {winner['Train RMSE']:.5f}\")\n",
    "if improved:\n",
    "    print(f\"NEW BEST! Beat previous {previous_best:.5f} by {(previous_best - winner['Test RMSE']):.5f}\")\n",
    "    save_best_score(winner[\"Test RMSE\"])\n",
    "else:\n",
    "    print(f\"No improvement (previous best: {previous_best:.5f})\")\n",
    "print(\"█\" * 110)\n",
    "\n",
    "# Save full results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv(f\"benchmark_results_{timestamp}.csv\")\n",
    "print(f\"\\nFull results saved: benchmark_results_{timestamp}.csv\")\n",
    "history_df.to_csv(HIST_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fcaa7",
   "metadata": {},
   "source": [
    "## SGDRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05a816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SGDRegressor Params: {'sgd__alpha': 0.01, 'sgd__eta0': 0.1, 'sgd__learning_rate': 'adaptive', 'sgd__loss': 'huber'}\n",
      "Best CV RMSE: 0.13423956433482814\n",
      "Test RMSE: 0.1239647010302294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Pipeline: Scaling + SGD Regressor\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"sgd\", SGDRegressor(max_iter=5000, tol=1e-3, random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"sgd__alpha\": [1e-4, 1e-3, 1e-2],        # Regularization strength\n",
    "    \"sgd__learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"sgd__eta0\": [0.01, 0.1, 0.5],           # Initial learning rate\n",
    "    \"sgd__loss\": [\"squared_error\", \"huber\"]  # Loss function\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid, scoring=rmse_scorer, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"Best SGDRegressor Params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5c1df",
   "metadata": {},
   "source": [
    "## Spline regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7e0189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Spline Params: {'spline__degree': 3, 'spline__include_bias': False, 'spline__n_knots': 5}\n",
      "Best CV RMSE: 0.23676842402469864\n",
      "Test RMSE: 0.4101220648690916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import SplineTransformer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target if needed\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                           greater_is_better=False)\n",
    "\n",
    "# Pipeline: Spline transformation + Linear Regression\n",
    "pipe = Pipeline([\n",
    "    (\"spline\", SplineTransformer()),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"spline__degree\": [3],           # cubic spline\n",
    "    \"spline__n_knots\": [5, 10, 15], # number of knots\n",
    "    \"spline__include_bias\": [False] # don't include constant term\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(pipe, param_grid, scoring=rmse_scorer, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"Best Spline Params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a8c56",
   "metadata": {},
   "source": [
    "## Poisson Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89be3366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'poisson__alpha': 10.0, 'poisson__fit_intercept': True}\n",
      "Best CV RMSE: 32313.8184125024\n",
      "Test RMSE: 0.12757021955405573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# ========== Target ==========\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "# ========== Pipeline ==========\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poisson\", PoissonRegressor(max_iter=5000))\n",
    "])\n",
    "\n",
    "# ========== Hyperparameters ==========\n",
    "param_grid = {\n",
    "    \"poisson__alpha\": np.logspace(-4, 1, 10),     # regularization\n",
    "    \"poisson__fit_intercept\": [True, False]\n",
    "}\n",
    "\n",
    "# ========== RMSE scorer ==========\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# ========== GridSearch ==========\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ⚠️ FIT ON RAW y, NOT LOG\n",
    "grid.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# ========== Evaluate ==========\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, np.log1p(y_pred_test)))\n",
    "\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9017ff",
   "metadata": {},
   "source": [
    "## Neural Network Regression (MLPRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d7c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.01}\n",
      "Best CV RMSE: 0.13625146233540686\n",
      "Test RMSE: 0.1392120628979391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Base MLP model\n",
    "mlp = MLPRegressor(\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3],\n",
    "    \"learning_rate_init\": [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Test evaluation\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f63b2",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression (KRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5e1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'krr__alpha': 0.1, 'krr__degree': 2, 'krr__gamma': 0.01, 'krr__kernel': 'polynomial'}\n",
      "Best CV RMSE: 0.1649557616593605\n",
      "Test RMSE: 0.13911764097684137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Pipeline: scaling + kernel ridge regression\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"krr\", KernelRidge())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"krr__alpha\": [1e-2, 1e-1, 1, 10],            # regularization\n",
    "    \"krr__kernel\": [\"linear\", \"polynomial\", \"rbf\"], \n",
    "    \"krr__degree\": [2, 3, 4],                     # only for polynomial kernel\n",
    "    \"krr__gamma\": [0.01, 0.1, 1, 10]             # only for rbf/poly kernel\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ac24b",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04a9d7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Polynomial Degree: {'poly__degree': 2}\n",
      "Best CV RMSE: 0.15299423296688647\n",
      "Test RMSE: 0.15548587765040164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "# Pipeline: Polynomial features + scaling + linear regression\n",
    "pipe = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid: degree of polynomial\n",
    "param_grid = {\n",
    "    \"poly__degree\": [2],  # try 2nd, 3rd, 4th degree\n",
    "}\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"Best Polynomial Degree:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490e974",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9614182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'svr__C': 1, 'svr__epsilon': 0.05, 'svr__gamma': 'auto'}\n",
      "Best CV RMSE: 0.18884353563650363\n",
      "Test RMSE: 0.1878527824521924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "# Pipeline: scale + SVR\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    \"svr__C\": [0.1, 1, 10, 100],\n",
    "    \"svr__epsilon\": [0.05, 0.1, 0.2],\n",
    "    \"svr__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit SVR model\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac54ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVR Params: {'svr__C': 100, 'svr__epsilon': 0.05, 'svr__kernel': 'linear'}\n",
      "Best CV RMSE: 0.1340834593893518\n",
      "Test RMSE: 0.12638908333891602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "# Scale + SVR pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "# Heuristic for RBF gamma: midpoint between 10th and 90th percentile of squared distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "dists = euclidean_distances(X_train_final, X_train_final)\n",
    "dists_squared = dists ** 2\n",
    "gamma_rbf_guess = 1 / np.median(dists_squared)  # rough estimate (can refine)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"svr__kernel\": [\"linear\"],\n",
    "        \"svr__C\": [0.1, 1, 10, 100],\n",
    "        \"svr__epsilon\": [0.05, 0.1, 0.2]\n",
    "    },\n",
    "    {\n",
    "        \"svr__kernel\": [\"rbf\"],\n",
    "        \"svr__C\": [1, 10, 100],\n",
    "        \"svr__epsilon\": [0.05, 0.1],\n",
    "        \"svr__gamma\": [gamma_rbf_guess / 2, gamma_rbf_guess, gamma_rbf_guess * 2]\n",
    "    },\n",
    "    {\n",
    "        \"svr__kernel\": [\"poly\"],\n",
    "        \"svr__C\": [1, 10],\n",
    "        \"svr__epsilon\": [0.05, 0.1],\n",
    "        \"svr__degree\": [2, 3, 4],\n",
    "        \"svr__gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    {\n",
    "        \"svr__kernel\": [\"sigmoid\"],\n",
    "        \"svr__C\": [1, 10],\n",
    "        \"svr__epsilon\": [0.05, 0.1],\n",
    "        \"svr__gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit SVR\n",
    "grid.fit(X_train_final, y_train_log)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best SVR Params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"Test RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d921f90",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1f4b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "Fold RMSE: 0.1368\n",
      "\n",
      "===== Fold 2 =====\n",
      "Fold RMSE: 0.1159\n",
      "\n",
      "===== Fold 3 =====\n",
      "Fold RMSE: 0.1290\n",
      "\n",
      "===== Fold 4 =====\n",
      "Fold RMSE: 0.1352\n",
      "\n",
      "===== Fold 5 =====\n",
      "Fold RMSE: 0.1599\n",
      "\n",
      "Average CV RMSE: 0.13535158127786515\n",
      "Test RMSE (log scale): 0.12734955094874753\n",
      "Test RMSE (original scale): 26458.972920353503\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Log-transform target\n",
    "# -----------------------------\n",
    "y_train_log = np.log1p(y_train.squeeze())\n",
    "y_test_log = np.log1p(y_test.squeeze())\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameters for small dataset\n",
    "# -----------------------------\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_alpha\": 1,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"gamma\": 0,\n",
    "    \"n_estimators\": 2000,   # fixed boosting rounds\n",
    "    \"verbosity\": 1,\n",
    "    \"tree_method\": \"hist\",  # fast\n",
    "    \"random_state\": 42\n",
    "    # You can add monotone_constraints=[...] if needed\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# K-Fold CV\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = []\n",
    "fold_rmse = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final)):\n",
    "    print(f\"\\n===== Fold {fold+1} =====\")\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    # No early stopping\n",
    "    model.fit(X_tr, y_tr, verbose=50)\n",
    "\n",
    "    models.append(model)\n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    fold_rmse.append(rmse)\n",
    "    print(f\"Fold RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"\\nAverage CV RMSE:\", np.mean(fold_rmse))\n",
    "\n",
    "# -----------------------------\n",
    "# Predict test set using average of fold models\n",
    "# -----------------------------\n",
    "test_pred_log = np.mean([m.predict(X_test_final) for m in models], axis=0)\n",
    "test_rmse_log = np.sqrt(mean_squared_error(y_test_log, test_pred_log))\n",
    "print(\"Test RMSE (log scale):\", test_rmse_log)\n",
    "\n",
    "# Convert back to original scale\n",
    "test_pred = np.expm1(test_pred_log)\n",
    "test_rmse_orig = np.sqrt(mean_squared_error(y_test.squeeze(), test_pred))\n",
    "print(\"Test RMSE (original scale):\", test_rmse_orig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491b0a2",
   "metadata": {},
   "source": [
    "#### XGBoost Tweedie Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec5e35",
   "metadata": {},
   "source": [
    "### XGBoost Fair Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db91c8",
   "metadata": {},
   "source": [
    "### LightGbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac2471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best LightGBM Params: {'num_leaves': 31, 'n_estimators': 200, 'min_child_samples': 50, 'max_depth': 3, 'learning_rate': 0.1, 'lambda_l2': 10, 'lambda_l1': 1, 'feature_fraction': 1.0, 'bagging_freq': 1, 'bagging_fraction': 1.0}\n",
      "Best CV RMSE: 0.13935702876975375\n",
      "Test RMSE (log scale): 0.1304644670756598\n",
      "Test RMSE (original scale): 27807.402577616966\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Log-transform target\n",
    "# -----------------------------\n",
    "y_train_log = np.log1p(y_train.squeeze())\n",
    "y_test_log = np.log1p(y_test.squeeze())\n",
    "\n",
    "# -----------------------------\n",
    "# Categorical columns as category dtype\n",
    "# -----------------------------\n",
    "for col in X_train_final.select_dtypes('object').columns:\n",
    "    X_train_final[col] = X_train_final[col].astype('category')\n",
    "    X_test_final[col] = X_test_final[col].astype('category')\n",
    "\n",
    "# -----------------------------\n",
    "# RMSE scorer\n",
    "# -----------------------------\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# LightGBM regressor\n",
    "# -----------------------------\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameter grid (small dataset guidance)\n",
    "# -----------------------------\n",
    "param_distributions = {\n",
    "    \"num_leaves\": [20, 31, 40, 50],\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"learning_rate\": [0.05, 0.08, 0.1],\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"min_child_samples\": [50, 75, 100],\n",
    "    \"lambda_l1\": [1, 5, 10],\n",
    "    \"lambda_l2\": [1, 5, 10],\n",
    "    \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "    \"bagging_fraction\": [0.8, 0.9, 1.0],\n",
    "    \"bagging_freq\": [1]  # enable bagging\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Randomized search with 5-fold CV\n",
    "# -----------------------------\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # number of random combinations to try\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Fit model\n",
    "# -----------------------------\n",
    "random_search.fit(X_train_final, y_train_log)\n",
    "\n",
    "# -----------------------------\n",
    "# Best parameters and CV RMSE\n",
    "# -----------------------------\n",
    "print(\"Best LightGBM Params:\", random_search.best_params_)\n",
    "print(\"Best CV RMSE:\", -random_search.best_score_)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on test set\n",
    "# -----------------------------\n",
    "y_pred_test_log = random_search.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test_log))\n",
    "print(\"Test RMSE (log scale):\", test_rmse)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred_test = np.expm1(y_pred_test_log)\n",
    "test_rmse_orig = np.sqrt(mean_squared_error(y_test.squeeze(), y_pred_test))\n",
    "print(\"Test RMSE (original scale):\", test_rmse_orig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b5751",
   "metadata": {},
   "source": [
    "## RandomForestReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa768cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "cv_scores = -cross_val_score(rf, X_train_final, y_train_log, cv=5,\n",
    "                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "print(\"RF CV RMSE:\", cv_scores.mean())\n",
    "\n",
    "rf.fit(X_train_final, y_train_log)\n",
    "y_pred_test = rf.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"RF Test RMSE:\", test_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514242a",
   "metadata": {},
   "source": [
    "## GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                               max_depth=4, random_state=42)\n",
    "cv_scores = -cross_val_score(gb, X_train_scaled, y_train_log, cv=5,\n",
    "                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "print(\"GBM CV RMSE:\", cv_scores.mean())\n",
    "\n",
    "gb.fit(X_train_scaled, y_train_log)\n",
    "y_pred_test = gb.predict(X_test_scaled)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "print(\"GBM Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75bc8601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best LightGBM Params: {'num_leaves': 31, 'n_estimators': 200, 'min_child_samples': 50, 'max_depth': 3, 'learning_rate': 0.1, 'lambda_l2': 10, 'lambda_l1': 1, 'feature_fraction': 1.0, 'bagging_freq': 1, 'bagging_fraction': 1.0}\n",
      "Best CV RMSE: 0.13935702876975375\n",
      "Test RMSE (log scale): 0.1304644670756598\n",
      "Test RMSE (original scale): 27807.402577616966\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Log-transform target\n",
    "# -----------------------------\n",
    "y_train_log = np.log1p(y_train.squeeze())\n",
    "y_test_log = np.log1p(y_test.squeeze())\n",
    "\n",
    "# -----------------------------\n",
    "# Categorical columns as category dtype\n",
    "# -----------------------------\n",
    "for col in X_train_final.select_dtypes('object').columns:\n",
    "    X_train_final[col] = X_train_final[col].astype('category')\n",
    "    X_test_final[col] = X_test_final[col].astype('category')\n",
    "\n",
    "# -----------------------------\n",
    "# RMSE scorer\n",
    "# -----------------------------\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# LightGBM regressor\n",
    "# -----------------------------\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameter grid (small dataset guidance)\n",
    "# -----------------------------\n",
    "param_distributions = {\n",
    "    \"num_leaves\": [20, 31, 40, 50],\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"learning_rate\": [0.05, 0.08, 0.1],\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"min_child_samples\": [50, 75, 100],\n",
    "    \"lambda_l1\": [1, 5, 10],\n",
    "    \"lambda_l2\": [1, 5, 10],\n",
    "    \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "    \"bagging_fraction\": [0.8, 0.9, 1.0],\n",
    "    \"bagging_freq\": [1]  # enable bagging\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Randomized search with 5-fold CV\n",
    "# -----------------------------\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # number of random combinations to try\n",
    "    scoring=rmse_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Fit model\n",
    "# -----------------------------\n",
    "random_search.fit(X_train_final, y_train_log)\n",
    "\n",
    "# -----------------------------\n",
    "# Best parameters and CV RMSE\n",
    "# -----------------------------\n",
    "print(\"Best LightGBM Params:\", random_search.best_params_)\n",
    "print(\"Best CV RMSE:\", -random_search.best_score_)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on test set\n",
    "# -----------------------------\n",
    "y_pred_test_log = random_search.predict(X_test_final)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test_log))\n",
    "print(\"Test RMSE (log scale):\", test_rmse)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred_test = np.expm1(y_pred_test_log)\n",
    "test_rmse_orig = np.sqrt(mean_squared_error(y_test.squeeze(), y_pred_test))\n",
    "print(\"Test RMSE (original scale):\", test_rmse_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2740a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[100]\tvalid_0's rmse: 10.1088\n",
      "[200]\tvalid_0's rmse: 10.3726\n",
      "[300]\tvalid_0's rmse: 12.1261\n",
      "[400]\tvalid_0's rmse: 10.6488\n",
      "[500]\tvalid_0's rmse: 11.3268\n",
      "[600]\tvalid_0's rmse: 12.7434\n",
      "[700]\tvalid_0's rmse: 11.9515\n",
      "[800]\tvalid_0's rmse: 11.4003\n",
      "[900]\tvalid_0's rmse: 11.7801\n",
      "[1000]\tvalid_0's rmse: 11.9442\n",
      "[1100]\tvalid_0's rmse: 12.1784\n",
      "[1200]\tvalid_0's rmse: 12.0705\n",
      "[1300]\tvalid_0's rmse: 11.6512\n",
      "[1400]\tvalid_0's rmse: 11.6988\n",
      "[1500]\tvalid_0's rmse: 12.1663\n",
      "[1600]\tvalid_0's rmse: 12.4506\n",
      "[1700]\tvalid_0's rmse: 12.4331\n",
      "[1800]\tvalid_0's rmse: 12.3822\n",
      "[1900]\tvalid_0's rmse: 12.2581\n",
      "[2000]\tvalid_0's rmse: 12.2286\n",
      "Fold RMSE: 13.0569\n",
      "\n",
      "===== Fold 2 =====\n",
      "[100]\tvalid_0's rmse: 10.1182\n",
      "[200]\tvalid_0's rmse: 10.8871\n",
      "[300]\tvalid_0's rmse: 12.6559\n",
      "[400]\tvalid_0's rmse: 12.5192\n",
      "[500]\tvalid_0's rmse: 12.7685\n",
      "[600]\tvalid_0's rmse: 12.7934\n",
      "[700]\tvalid_0's rmse: 12.6599\n",
      "[800]\tvalid_0's rmse: 12.4095\n",
      "[900]\tvalid_0's rmse: 11.8762\n",
      "[1000]\tvalid_0's rmse: 12.0884\n",
      "[1100]\tvalid_0's rmse: 12.4647\n",
      "[1200]\tvalid_0's rmse: 11.8096\n",
      "[1300]\tvalid_0's rmse: 12.1001\n",
      "[1400]\tvalid_0's rmse: 12.1385\n",
      "[1500]\tvalid_0's rmse: 12.1499\n",
      "[1600]\tvalid_0's rmse: 12.3188\n",
      "[1700]\tvalid_0's rmse: 12.6089\n",
      "[1800]\tvalid_0's rmse: 12.6692\n",
      "[1900]\tvalid_0's rmse: 12.887\n",
      "[2000]\tvalid_0's rmse: 13.1873\n",
      "Fold RMSE: 13.5311\n",
      "\n",
      "===== Fold 3 =====\n",
      "[100]\tvalid_0's rmse: 10.085\n",
      "[200]\tvalid_0's rmse: 11.6491\n",
      "[300]\tvalid_0's rmse: 11.5712\n",
      "[400]\tvalid_0's rmse: 10.9732\n",
      "[500]\tvalid_0's rmse: 11.2911\n",
      "[600]\tvalid_0's rmse: 11.6813\n",
      "[700]\tvalid_0's rmse: 12.6482\n",
      "[800]\tvalid_0's rmse: 12.7611\n",
      "[900]\tvalid_0's rmse: 12.4573\n",
      "[1000]\tvalid_0's rmse: 12.1267\n",
      "[1100]\tvalid_0's rmse: 11.5414\n",
      "[1200]\tvalid_0's rmse: 10.8368\n",
      "[1300]\tvalid_0's rmse: 10.9375\n",
      "[1400]\tvalid_0's rmse: 11.0124\n",
      "[1500]\tvalid_0's rmse: 11.1077\n",
      "[1600]\tvalid_0's rmse: 11.2472\n",
      "[1700]\tvalid_0's rmse: 11.2739\n",
      "[1800]\tvalid_0's rmse: 11.4649\n",
      "[1900]\tvalid_0's rmse: 11.6048\n",
      "[2000]\tvalid_0's rmse: 11.9488\n",
      "Fold RMSE: 12.9955\n",
      "\n",
      "===== Fold 4 =====\n",
      "[100]\tvalid_0's rmse: 10.0931\n",
      "[200]\tvalid_0's rmse: 11.8089\n",
      "[300]\tvalid_0's rmse: 13.1746\n",
      "[400]\tvalid_0's rmse: 12.7363\n",
      "[500]\tvalid_0's rmse: 13.1905\n",
      "[600]\tvalid_0's rmse: 13.5123\n",
      "[700]\tvalid_0's rmse: 13.1645\n",
      "[800]\tvalid_0's rmse: 12.7466\n",
      "[900]\tvalid_0's rmse: 12.6126\n",
      "[1000]\tvalid_0's rmse: 12.392\n",
      "[1100]\tvalid_0's rmse: 11.6646\n",
      "[1200]\tvalid_0's rmse: 11.6221\n",
      "[1300]\tvalid_0's rmse: 11.8697\n",
      "[1400]\tvalid_0's rmse: 12.7813\n",
      "[1500]\tvalid_0's rmse: 13.0994\n",
      "[1600]\tvalid_0's rmse: 13.0256\n",
      "[1700]\tvalid_0's rmse: 12.9702\n",
      "[1800]\tvalid_0's rmse: 12.9424\n",
      "[1900]\tvalid_0's rmse: 12.9013\n",
      "[2000]\tvalid_0's rmse: 12.8564\n",
      "Fold RMSE: 10.2943\n",
      "\n",
      "===== Fold 5 =====\n",
      "[100]\tvalid_0's rmse: 9.84501\n",
      "[200]\tvalid_0's rmse: 10.2629\n",
      "[300]\tvalid_0's rmse: 12.1471\n",
      "[400]\tvalid_0's rmse: 11.1041\n",
      "[500]\tvalid_0's rmse: 11.1726\n",
      "[600]\tvalid_0's rmse: 11.99\n",
      "[700]\tvalid_0's rmse: 12.2846\n",
      "[800]\tvalid_0's rmse: 12.2216\n",
      "[900]\tvalid_0's rmse: 12.9771\n",
      "[1000]\tvalid_0's rmse: 12.8857\n",
      "[1100]\tvalid_0's rmse: 12.8568\n",
      "[1200]\tvalid_0's rmse: 12.786\n",
      "[1300]\tvalid_0's rmse: 12.7554\n",
      "[1400]\tvalid_0's rmse: 12.6918\n",
      "[1500]\tvalid_0's rmse: 12.5938\n",
      "[1600]\tvalid_0's rmse: 12.4447\n",
      "[1700]\tvalid_0's rmse: 12.3479\n",
      "[1800]\tvalid_0's rmse: 12.7432\n",
      "[1900]\tvalid_0's rmse: 12.8098\n",
      "[2000]\tvalid_0's rmse: 12.7288\n",
      "Fold RMSE: 10.8080\n",
      "\n",
      "Average CV RMSE: 12.137148289033998\n",
      "Test RMSE (log scale): 12.120983812535563\n",
      "Test RMSE (original scale): 198446.21489216897\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Log-transform target\n",
    "# -----------------------------\n",
    "y_train_log = np.log1p(y_train.squeeze())\n",
    "y_test_log = np.log1p(y_test.squeeze())\n",
    "\n",
    "# -----------------------------\n",
    "# Categorical columns as category dtype\n",
    "# -----------------------------\n",
    "for col in X_train_final.select_dtypes('object').columns:\n",
    "    X_train_final[col] = X_train_final[col].astype('category')\n",
    "    X_test_final[col] = X_test_final[col].astype('category')\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameters for small dataset with DART\n",
    "# -----------------------------\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boosting_type\": \"dart\",   # DART boosting\n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_child_samples\": 75,\n",
    "    \"lambda_l1\": 1,\n",
    "    \"lambda_l2\": 1,\n",
    "    \"drop_rate\": 0.1,          # fraction of trees dropped each iteration\n",
    "    \"verbose\": -1,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Number of boosting rounds\n",
    "# -----------------------------\n",
    "num_boost_round = 2000\n",
    "\n",
    "# -----------------------------\n",
    "# K-Fold CV\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = []\n",
    "fold_rmse = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final)):\n",
    "    print(f\"\\n===== Fold {fold+1} =====\")\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]\n",
    "\n",
    "    train_set = lgb.Dataset(X_tr, y_tr)\n",
    "    valid_set = lgb.Dataset(X_val, y_val, reference=train_set)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[valid_set],\n",
    "        callbacks=[lgb.log_evaluation(period=50)]  # Prints every 50 iterations\n",
    "    )\n",
    "\n",
    "    models.append(model)\n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    fold_rmse.append(rmse)\n",
    "    print(f\"Fold RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"\\nAverage CV RMSE:\", np.mean(fold_rmse))\n",
    "\n",
    "# -----------------------------\n",
    "# Predict test set using average of fold models\n",
    "# -----------------------------\n",
    "test_pred_log = np.mean([m.predict(X_test_final) for m in models], axis=0)\n",
    "test_rmse_log = np.sqrt(mean_squared_error(y_test_log, test_pred_log))\n",
    "print(\"Test RMSE (log scale):\", test_rmse_log)\n",
    "\n",
    "# Convert back to original scale\n",
    "test_pred = np.expm1(test_pred_log)\n",
    "test_rmse_orig = np.sqrt(mean_squared_error(y_test.squeeze(), test_pred))\n",
    "print(\"Test RMSE (original scale):\", test_rmse_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e7e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5c52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa4c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bb3c883",
   "metadata": {},
   "source": [
    "# Full Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a13979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Numeric columns to transform\n",
    "to_transform = X_train.select_dtypes(include=['int64','float64']).columns\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Lasso\": LassoCV(alphas=np.logspace(-4, 1, 10), cv=5, max_iter=5000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=500, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                                 max_depth=4, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=4,\n",
    "                                subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Pipeline: Yeo-Johnson + StandardScaler + Model\n",
    "    pipe = Pipeline([\n",
    "        (\"yeojohnson\", PowerTransformer(method='yeo-johnson')),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    # 5-fold CV RMSE\n",
    "    cv_rmse = -cross_val_score(pipe, X_train, y_train_log, cv=5,\n",
    "                               scoring='neg_root_mean_squared_error', n_jobs=-1).mean()\n",
    "    \n",
    "    # Fit on full training\n",
    "    pipe.fit(X_train, y_train_log)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test))\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV_RMSE\": cv_rmse,\n",
    "        \"Test_RMSE\": test_rmse\n",
    "    })\n",
    "\n",
    "# Compare\n",
    "results_df = pd.DataFrame(results).sort_values(by='Test_RMSE')\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
